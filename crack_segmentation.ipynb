{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP27s3MtB1JOUPtdB2Thx5q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doanthuan/wildfire-detection/blob/main/crack_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install this package to use Colab's GPU for training\n",
        "!apt install --allow-change-held-packages libcudnn8=8.4.1.50-1+cuda11.6"
      ],
      "metadata": {
        "id": "a_Lz0gcGMvGo",
        "outputId": "1b13b69a-3960-41a8-b459-63d3113d12fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following packages will be REMOVED:\n",
            "  libcudnn8-dev\n",
            "The following held packages will be changed:\n",
            "  libcudnn8\n",
            "The following packages will be upgraded:\n",
            "  libcudnn8\n",
            "1 upgraded, 0 newly installed, 1 to remove and 18 not upgraded.\n",
            "Need to get 420 MB of archives.\n",
            "After this operation, 3,369 MB disk space will be freed.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libcudnn8 8.4.1.50-1+cuda11.6 [420 MB]\n",
            "Fetched 420 MB in 6s (67.8 MB/s)\n",
            "(Reading database ... 155569 files and directories currently installed.)\n",
            "Removing libcudnn8-dev (8.0.5.39-1+cuda11.1) ...\n",
            "(Reading database ... 155547 files and directories currently installed.)\n",
            "Preparing to unpack .../libcudnn8_8.4.1.50-1+cuda11.6_amd64.deb ...\n",
            "Unpacking libcudnn8 (8.4.1.50-1+cuda11.6) over (8.0.5.39-1+cuda11.1) ...\n",
            "Setting up libcudnn8 (8.4.1.50-1+cuda11.6) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "RC5YvlCrNJ4W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "agM0CI0_youc",
        "outputId": "1c158459-6917-4334-ea9b-98ba2e4325bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the Dataset"
      ],
      "metadata": {
        "id": "xmV2nWPJNMif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1Kbvg6zLryzew4OHcHYdV4JnCqOpGe4LR -O /tmp/crack-dataset.zip "
      ],
      "metadata": {
        "id": "rB7fJtUvNNCy",
        "outputId": "96fc6722-b657-411d-b50c-483ce3177bcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Kbvg6zLryzew4OHcHYdV4JnCqOpGe4LR\n",
            "To: /tmp/crack-dataset.zip\n",
            "100% 1.06G/1.06G [00:08<00:00, 125MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the downloaded dataset to a local directory: /tmp/fcnn\n",
        "local_zip = '/tmp/crack-dataset.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/crack')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "bOLV6WgrS_IP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pixel labels in the video frames\n",
        "class_names = ['crack']"
      ],
      "metadata": {
        "id": "LBTWApnRTLEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Prepare the Dataset"
      ],
      "metadata": {
        "id": "N1ChkMXjTR4m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, you will load and prepare the train and validation sets for training. There are some preprocessing steps needed before the data is fed to the model. These include:\n",
        "\n",
        "* resizing the height and width of the input images and label maps (224 x 224px by default)\n",
        "* normalizing the input images' pixel values to fall in the range `[-1, 1]`\n",
        "* reshaping the label maps from `(height, width, 1)` to `(height, width, 12)` with each slice along the third axis having `1` if it belongs to the class corresponding to that slice's index else `0`. For example, if a pixel is part of a road, then using the table above, that point at slice #3 will be labeled `1` and it will be `0` in all other slices. To illustrate using simple arrays:\n",
        "```\n",
        "# if we have a label map with 3 classes...\n",
        "n_classes = 3\n",
        "# and this is the original annotation...\n",
        "orig_anno = [0 1 2]\n",
        "# then the reshaped annotation will have 3 slices and its contents will look like this:\n",
        "reshaped_anno = [1 0 0][0 1 0][0 0 1]\n",
        "```\n",
        "\n",
        "The following function will do the preprocessing steps mentioned above."
      ],
      "metadata": {
        "id": "9llRcGf3TYBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_filename_to_image_and_mask(t_filename, a_filename, height=224, width=224):\n",
        "  '''\n",
        "  Preprocesses the dataset by:\n",
        "    * resizing the input image and label maps\n",
        "    * normalizing the input image pixels\n",
        "    * reshaping the label maps from (height, width, 1) to (height, width, 12)\n",
        "\n",
        "  Args:\n",
        "    t_filename (string) -- path to the raw input image\n",
        "    a_filename (string) -- path to the raw annotation (label map) file\n",
        "    height (int) -- height in pixels to resize to\n",
        "    width (int) -- width in pixels to resize to\n",
        "\n",
        "  Returns:\n",
        "    image (tensor) -- preprocessed image\n",
        "    annotation (tensor) -- preprocessed annotation\n",
        "  '''\n",
        "\n",
        "  # Convert image and mask files to tensors \n",
        "  img_raw = tf.io.read_file(t_filename)\n",
        "  anno_raw = tf.io.read_file(a_filename)\n",
        "  image = tf.image.decode_jpeg(img_raw)\n",
        "  annotation = tf.image.decode_jpeg(anno_raw)\n",
        " \n",
        "  # Resize image and segmentation mask\n",
        "  image = tf.image.resize(image, (height, width,))\n",
        "  annotation = tf.image.resize(annotation, (height, width,))\n",
        "  image = tf.reshape(image, (height, width, 3,))\n",
        "  annotation = tf.cast(annotation, dtype=tf.int32)\n",
        "  annotation = tf.reshape(annotation, (height, width, 1,))\n",
        "  stack_list = []\n",
        "\n",
        "  # Reshape segmentation masks\n",
        "  for c in range(len(class_names)):\n",
        "    mask = tf.equal(annotation[:,:,0], tf.constant(c))\n",
        "    stack_list.append(tf.cast(mask, dtype=tf.int32))\n",
        "  \n",
        "  annotation = tf.stack(stack_list, axis=2)\n",
        "\n",
        "  # Normalize pixels in the input image\n",
        "  image = image/127.5\n",
        "  image -= 1\n",
        "\n",
        "  return image, annotation"
      ],
      "metadata": {
        "id": "aJR_9TVBTTLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilities for preparing the datasets\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "def get_dataset_slice_paths(image_dir, label_map_dir):\n",
        "  '''\n",
        "  generates the lists of image and label map paths\n",
        "  \n",
        "  Args:\n",
        "    image_dir (string) -- path to the input images directory\n",
        "    label_map_dir (string) -- path to the label map directory\n",
        "\n",
        "  Returns:\n",
        "    image_paths (list of strings) -- paths to each image file\n",
        "    label_map_paths (list of strings) -- paths to each label map\n",
        "  '''\n",
        "  image_file_list = os.listdir(image_dir)\n",
        "  label_map_file_list = os.listdir(label_map_dir)\n",
        "  image_paths = [os.path.join(image_dir, fname) for fname in image_file_list]\n",
        "  label_map_paths = [os.path.join(label_map_dir, fname) for fname in label_map_file_list]\n",
        "\n",
        "  return image_paths, label_map_paths\n",
        "\n",
        "\n",
        "def get_training_dataset(image_paths, label_map_paths):\n",
        "  '''\n",
        "  Prepares shuffled batches of the training set.\n",
        "  \n",
        "  Args:\n",
        "    image_paths (list of strings) -- paths to each image file in the train set\n",
        "    label_map_paths (list of strings) -- paths to each label map in the train set\n",
        "\n",
        "  Returns:\n",
        "    tf Dataset containing the preprocessed train set\n",
        "  '''\n",
        "  training_dataset = tf.data.Dataset.from_tensor_slices((image_paths, label_map_paths))\n",
        "  training_dataset = training_dataset.map(map_filename_to_image_and_mask)\n",
        "  training_dataset = training_dataset.shuffle(100, reshuffle_each_iteration=True)\n",
        "  training_dataset = training_dataset.batch(BATCH_SIZE)\n",
        "  training_dataset = training_dataset.repeat()\n",
        "  training_dataset = training_dataset.prefetch(-1)\n",
        "\n",
        "  return training_dataset\n",
        "\n",
        "\n",
        "def get_validation_dataset(image_paths, label_map_paths):\n",
        "  '''\n",
        "  Prepares batches of the validation set.\n",
        "  \n",
        "  Args:\n",
        "    image_paths (list of strings) -- paths to each image file in the val set\n",
        "    label_map_paths (list of strings) -- paths to each label map in the val set\n",
        "\n",
        "  Returns:\n",
        "    tf Dataset containing the preprocessed validation set\n",
        "  '''\n",
        "  validation_dataset = tf.data.Dataset.from_tensor_slices((image_paths, label_map_paths))\n",
        "  validation_dataset = validation_dataset.map(map_filename_to_image_and_mask)\n",
        "  validation_dataset = validation_dataset.batch(BATCH_SIZE)\n",
        "  validation_dataset = validation_dataset.repeat()  \n",
        "\n",
        "  return validation_dataset\n"
      ],
      "metadata": {
        "id": "T8-GQkj_TnPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0OBMhdctTpdg"
      }
    }
  ]
}